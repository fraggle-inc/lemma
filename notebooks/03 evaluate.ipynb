{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import regex\n",
    "import unicodecsv as csv\n",
    "import lemma\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMS_FILE = \"./data/norms.csv\"\n",
    "UD_TRAIN_FILE = \"./data/UD_Danish/da-ud-train.conllu\"\n",
    "UD_DEV_FILE = \"./data/UD_Danish/da-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the normalization rules we build in the first notebook. When evaluating, we apply these to the lemmas specified in UD. Otherwise we would risk, for example, counting \"akvarie\" as the incorrect lemma for \"akvarier\" if UD specified the other spelling, \"akvarium\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_lookup = dict([row for row in csv.reader(open(NORMS_FILE, 'rb'), delimiter=\",\",\n",
    "                        quotechar='\"',\n",
    "                        quoting=csv.QUOTE_MINIMAL,\n",
    "                        encoding='utf-8',\n",
    "                        lineterminator='\\n')][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a few more normalization rules. This is due to DSN and UD not agreeing on the lemmas for certain words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_dsn_normalization = (('PRON', 'det', 'det', 'den'),\n",
    "                        ('ADJ', 'flere', 'mange', 'flere'),\n",
    "                        ('ADJ', 'mere', 'meget', 'mere'),\n",
    "                        ('ADJ', 'meget', 'meget', 'megen'),\n",
    "                        ('ADJ', 'fleste', 'mange', 'flest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemma.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_ud_line(line):\n",
    "    return line.split(\"\\t\")[1:4]\n",
    "\n",
    "def _evaluate(ud_file):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    ambiguous = 0\n",
    "    mistakes = {}\n",
    "    ambiguities = {}\n",
    "    pos_prev = \"\"\n",
    "    for line in open(ud_file).readlines():\n",
    "        if line.startswith(\"#\") or line.strip() == \"\":\n",
    "            pos_prev = \"\"\n",
    "            continue\n",
    "\n",
    "        orth, lemma_expected, pos = _parse_ud_line(line)\n",
    "\n",
    "        if pos == \"NOUN\" and lemma_expected in norm_lookup:\n",
    "            lemma_expected = norm_lookup[lemma_expected]\n",
    "        else:\n",
    "            for pos_, orth_, expected_ud, expected_dsn in ud_dsn_normalization:\n",
    "                if pos != pos_ or orth.lower() != orth_ or lemma_expected != expected_ud:\n",
    "                    continue            \n",
    "                lemma_expected = expected_dsn\n",
    "\n",
    "        lemmas_actual = lemmatizer.lemmatize(pos, orth.lower(), pos_previous=pos_prev)    \n",
    "        lemma_actual = lemmas_actual[0]\n",
    "\n",
    "        if len(lemmas_actual) > 1:\n",
    "            ambiguous += 1\n",
    "            ambiguities[(pos, orth)] = ambiguities.get((pos, orth), 0) + 1\n",
    "        elif lemma_actual.lower() == lemma_expected.lower():\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes[(pos, orth, lemma_expected, lemma_actual)] = mistakes.get((pos, orth, lemma_expected, lemma_actual), 0) + 1\n",
    "            incorrect += 1\n",
    "        pos_prev = pos\n",
    "\n",
    "    print(\"* correct:\", correct)\n",
    "    print(\"* incorrect:\", incorrect)\n",
    "    print(\"* ambiguous:\", ambiguous)\n",
    "    print(\"*\", correct/(incorrect+ambiguous+correct))\n",
    "    print(\"*\", (correct+ambiguous)/(incorrect+ambiguous+correct))\n",
    "    \n",
    "    return mistakes, ambiguities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on UD Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_train, ambiguities_train = _evaluate(UD_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on UD Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_dev, ambiguities_dev =_evaluate(UD_DEV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(mistakes_train.items(), key=lambda x: (-x[1], x[0][1].lower(), x))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ambiguities_train.items(), key=lambda x: (-x[1], x[0][1].lower(), x))[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
