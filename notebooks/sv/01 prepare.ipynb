{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Training\n",
    "In this notebook, we prepare a dataset which can be used for training a Swedish lemmatizer with Lemmy.\n",
    "\n",
    "**NOTE**: You do *not* need to run this notebook to use lemma. The lemmatizer comes trained and ready to use! This notebook is only if you want train the lemmatizer yourself, for example because you want it trained on a specific dataset.\n",
    "\n",
    "We use two datasets which are buth publicly available. One is [LEXIN](https://spraakbanken.gu.se/swe/resurser/nerladdning) and the other is one of the Swedish parts of the Universal Dependencies (UD). This dataset is open source and available from the [UD repo](https://github.com/UniversalDependencies/UD_Swedish-Talbanken) on GitHub.\n",
    "\n",
    "The notebook assumes you have the datasets stored in a subfolder called *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import unicodecsv as csv\n",
    "from tqdm import tqdm\n",
    "import regex\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s : %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_TRAIN_FILE = \"./data/UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu\" # da-ud-train.conllu # sv_talbanken-ud-train.conllu\n",
    "LEXIN_XML_FILE = \"./data/LEXIN/LEXIN.xml\"\n",
    "SALDOM_XML_FILE = \"./data/Saldom/saldom.xml\"\n",
    "PREPARED_FILE = \"./data/prepared.csv\"\n",
    "NORMS_FILE = \"./data/norms.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Lexin XML data\n",
    "Our first step reading the LEXIN data. We train the lemmatizer to use POS tags to help predict the lemma. We use the UD set of POS tags. Because the word classes used in DSN data differ from UD POS tags, we need to do some manual mapping. The `CLASS_LOOKUP` dictionary specifies the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LOOKUP = {\"subst.\": \"NOUN\",\n",
    "                \"verb\": \"VERB\",\n",
    "                \"adj.\": \"ADJ\",\n",
    "                \"adv.\": \"ADV\",\n",
    "                \"namn\": \"PROPN\",\n",
    "                \"interj.\": \"INTJ\",\n",
    "                \"räkn.\": \"NUM\",\n",
    "                \"pron.\": \"PRON\",\n",
    "                \"konj.\": \"CONJ\",\n",
    "                \"prep.\": \"ADP\"\n",
    "               }\n",
    "\n",
    "def _build_lexin_tuples(soup):\n",
    "    unknown_classes = defaultdict(int)\n",
    "    forms = set()\n",
    "    entries = soup.find_all('lemma-entry', recursive=True)\n",
    "    for entry in tqdm(entries):\n",
    "        lemma = entry.form.get_text()\n",
    "        if \" \" in lemma:\n",
    "            continue\n",
    "            \n",
    "        # Some entries in LEXIN are too far from being UD compatible.\n",
    "        # We simply skip them and expect to learn them from UD data.\n",
    "        if lemma == 'är':\n",
    "            continue\n",
    "        \n",
    "        if lemma != 'är':\n",
    "            pass\n",
    "            # continue\n",
    "\n",
    "        if \"~\" in lemma:\n",
    "            temp = lemma.split(\"~\")\n",
    "            prefix = temp[0]\n",
    "            lemma = \"\".join(temp)\n",
    "        else:\n",
    "            prefix = None\n",
    "\n",
    "        if not entry.pos:\n",
    "            continue\n",
    "\n",
    "        word_class = entry.pos.get_text()\n",
    "        if word_class not in CLASS_LOOKUP:\n",
    "            unknown_classes[word_class] += 1\n",
    "            continue\n",
    "        word_class = CLASS_LOOKUP[word_class]\n",
    "           \n",
    "\n",
    "        full_forms = entry.inflection.get_text().strip()\n",
    "        full_forms = full_forms.replace(\"(!)\", \"\")\n",
    "        if full_forms == \"explicit a\":\n",
    "            full_forms = [\"explicit\"]\n",
    "        elif full_forms:\n",
    "            full_forms = _parse_inflections(full_forms)\n",
    "        elif lemma.endswith(\"(s)\") or lemma.endswith(\"(a)\"):            \n",
    "            # For some pronouns, the lemma in ends with '(s)' or '(a)'.\n",
    "            lemma, temp = _process_optional(lemma)            \n",
    "            full_forms = [temp]\n",
    "        else:\n",
    "            full_forms = []\n",
    "\n",
    "\n",
    "        temp = []\n",
    "        for form in full_forms:\n",
    "            if form.startswith(\"-\"):\n",
    "                if prefix:\n",
    "                    temp.append(prefix + form[1:])\n",
    "                else:\n",
    "                    applied = _with_suffix(lemma, form[1:])\n",
    "                    if applied:\n",
    "                        temp.append(applied)\n",
    "                    else: \n",
    "                        temp.append(lemma + form[1:])\n",
    "            else:\n",
    "                if prefix:\n",
    "                    applied = _with_suffix(lemma, form)\n",
    "                    if applied:\n",
    "                        temp.append(applied)\n",
    "                    else: \n",
    "                        temp.append(lemma + form)\n",
    "\n",
    "                else:\n",
    "                    temp.append(form)\n",
    "        full_forms = temp\n",
    "\n",
    "        if word_class == \"VERB\":\n",
    "            # For verbs, the LEXIN data do not contain the lemma as the lookup word ('form')\n",
    "            # Instead, it's the last of the inflections, so we must do some swapping            \n",
    "            full_forms = [lemma] + full_forms\n",
    "            lemma = full_forms[-1]\n",
    "            full_forms = full_forms[:-1]\n",
    "        else:\n",
    "            full_forms.append(lemma)       \n",
    "        \n",
    "        for full_form in full_forms:\n",
    "            forms.add((word_class, full_form, lemma))\n",
    "    return sorted(forms, key = lambda x: x[1:]), unknown_classes\n",
    "\n",
    "def _parse_inflections(inflections):\n",
    "    temp = regex.sub(\"(\\((el\\.|eller|vard. även|el))(.+?)(\\))\", r'\\g<3>', inflections)\n",
    "    result = []\n",
    "    for form in temp.split():\n",
    "        result.extend(_process_optional(form))\n",
    "    return result\n",
    "\n",
    "def _process_optional(form):\n",
    "    m = regex.finditer(\"(.*)\\((.*)\\)(.*)\", form).match()\n",
    "    if not m:\n",
    "        return [form]\n",
    "    return [m.group(1) + m.group(3), m.group(1) + m.group(2) + m.group(3)]\n",
    "\n",
    "def _with_suffix(lemma, suffix, allow_recursions=3):\n",
    "    temp_suffix = suffix\n",
    "    while len(temp_suffix):\n",
    "        # print(temp_suffix)\n",
    "        if lemma.endswith(temp_suffix):\n",
    "            return lemma[:-len(temp_suffix)] + suffix\n",
    "        temp_suffix = temp_suffix[:-1]\n",
    "    \n",
    "    if allow_recursions > 0:\n",
    "        return _with_suffix(lemma[:-1], suffix, allow_recursions-1)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XML and parse it using Beautiful Soup\n",
    "soup = BeautifulSoup(open(LEXIN_XML_FILE, 'rb'), 'xml', from_encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19718/19718 [00:02<00:00, 7232.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build tuples of POS, + full form* and *lemma*\n",
    "lexin_tuples, unknown = _build_lexin_tuples(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 'A-inkomst', 'A-inkomst'),\n",
       " ('NOUN', 'A-inkomsten', 'A-inkomst'),\n",
       " ('NOUN', 'A-inkomster', 'A-inkomst'),\n",
       " ('NOUN', 'A-kassa', 'A-kassa'),\n",
       " ('NOUN', 'A-kassan', 'A-kassa')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexin_tuples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Saldom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XML and parse it using Beautiful Soup\n",
    "soup = BeautifulSoup(open(SALDOM_XML_FILE), 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LOOKUP = {\"vb\": \"VERB\",\n",
    "                \"nn\": \"NOUN\",\n",
    "                \"av\": \"ADJ\",\n",
    "                \"pm\": \"PROPN\",\n",
    "                \"ab\": \"ADV\",\n",
    "                \"nna\": \"NOUN\",\n",
    "                \"pp\": \"ADP\",\n",
    "                \"pma\": \"PROPN\",\n",
    "                \"in\": \"INTJ\",\n",
    "                \"nl\": \"NUM\",\n",
    "                \"pn\": \"PRON\",\n",
    "                \"sn\": \"CCONJ\",\n",
    "                \"aba\": \"ADV\",\n",
    "                \"ava\": \"ADJ\",\n",
    "                \"ppa\": \"ADP\",\n",
    "                \"kn\": \"CONJ\",\n",
    "                \"kna\": \"CONJ\",\n",
    "                \"vba\": \"VERB\",\n",
    "                \"al\": \"DET\",\n",
    "               }\n",
    "\n",
    "def _build_saldom_tuples(soup):\n",
    "    unknown_classes = defaultdict(int)\n",
    "    forms = set()\n",
    "    entries = soup.find_all('LexicalEntry', recursive=False) # rec True?\n",
    "    \n",
    "    for entry in tqdm(soup.find(\"LexicalResource\").find(\"Lexicon\").find_all('LexicalEntry', recursive=False)):        \n",
    "        lemma_feats = entry.find(\"Lemma\").find(\"FormRepresentation\").find_all(\"feat\")\n",
    "        lookup = { feat['att']: feat['val'] for feat in lemma_feats }\n",
    "        lemma = lookup['writtenForm']\n",
    "        if \" \" in lemma:\n",
    "            continue\n",
    "        word_class = lookup['partOfSpeech']\n",
    "        if word_class not in CLASS_LOOKUP:\n",
    "            unknown_classes[word_class] += 1\n",
    "            continue\n",
    "        word_class = CLASS_LOOKUP[word_class]\n",
    "\n",
    "        #if lemma == \"mången\" and word_class == \"PRON\":\n",
    "        #    word_class = \"ADJ\"\n",
    "        \n",
    "        full_forms = [feat[\"val\"] for form in entry.find_all(\"WordForm\") for feat in form.find_all(\"feat\") if feat[\"att\"] == \"writtenForm\"]\n",
    "        for full_form in full_forms:\n",
    "            forms.add((word_class, full_form, lemma))\n",
    "    return sorted(forms, key = lambda x: x[1:]), unknown_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128036/128036 [00:27<00:00, 4696.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build tuples of POS, + full form* and *lemma*\n",
    "saldom_tuples, unknown = _build_saldom_tuples(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', '%', '%'),\n",
       " ('NOUN', '%-', '%'),\n",
       " ('CONJ', '&', '&'),\n",
       " ('ADP', '+', '+'),\n",
       " ('ADP', '-', '-')]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saldom_tuples[:5]\n",
    "#[t for t in saldom_tuples if t[0] == 'CCONJ'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in saldom_tuples if t[0] == 'ADJ' and t[1] == 'många']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxc', 169), ('mxc', 24), ('avh', 15), ('nnh', 12), ('abh', 1)]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(unknown.keys())\n",
    "sorted(unknown.items(), key=lambda x: -1*x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse UD data\n",
    "We will now read the UD data.\n",
    "\n",
    "Some of the UD POS tags, such as *DET* and *AUX*, can not be mapped 1-to-1 to the DSN word classes. Consequently, we learn the words with those POS tags from UD.\n",
    "\n",
    "Since the UD data is not just a word list but actual sentences annotated with lemmas and POS tags (and more), we have the benefit of having not only the POS tag of the word we want to lemmatize, but also the POS tag of the previous word. We can use this to improve the accuracy of our lemmatizer, so when building the list of tuples from UD, we include the POS tag of the previous word of the sentence. This is set to the empty string when the current word is the first word of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_ud_line(line):\n",
    "    return line.split(\"\\t\")[1:4]\n",
    "\n",
    "def _build_ud_tuples(ud_file, min_freq=1):\n",
    "    counts = {}\n",
    "    pos_prev = \"\"\n",
    "    for line in open(ud_file).readlines():\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.strip() == \"\":\n",
    "            pos_prev = \"\"\n",
    "            continue\n",
    "\n",
    "        orth, lemma, pos = _parse_ud_line(line)\n",
    "        orth = orth.lower()\n",
    "        lemma = lemma.lower()\n",
    "        if pos == 'ADJ' and orth == 'många' and lemma == 'mången':\n",
    "            lemma = 'många'\n",
    "        key = (pos_prev, pos, orth, lemma)\n",
    "        counts[key] = counts.get(key, 0) + 1\n",
    "        pos_prev = pos\n",
    "    \n",
    "    return [key for key in counts if counts[key] >= min_freq]\n",
    "\n",
    "ud_tuples = _build_ud_tuples(UD_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'ADJ', 'individuell', 'individuell'),\n",
       " ('ADJ', 'NOUN', 'beskattning', 'beskattning'),\n",
       " ('NOUN', 'ADP', 'av', 'av'),\n",
       " ('ADP', 'NOUN', 'arbetsinkomster', 'arbetsinkomst'),\n",
       " ('', 'ADP', 'genom', 'genom')]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud_tuples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in ud_tuples if t[1] == 'ADJ' and t[2] == 'många' and t[3] == 'mången']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter UD data\n",
    "We will now filter the word forms read from UD. We do this to avoid introducing ambiguity due to spelling errors and typos in UD.\n",
    "\n",
    "We want to include the following only:\n",
    "1. Any POS + full form combination *not* found in DSN.\n",
    "2. Any POS_PREV + POS + full form combination for which the POS + full form is *ambiguous* in DSN + Step 1.\n",
    "\n",
    "By *ambiguous* we mean full forms (or combinations of POS tags and full forms) which have more than one lemma associated with them, which cause the lemmatizer to not know which of the lemmas to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set for looking up POS + full form combinations found in DSN.\n",
    "#lexin_full_forms = set((pos, full_form) for pos, full_form, _lemma in lexin_tuples)\n",
    "\n",
    "# Create a list of POS + full form + lemma tuples from UD for wich the POS + full form combination\n",
    "# is *not* found in DSN.\n",
    "#ud_tuples_unique = [(pos, full_form, lemma) for (_pos_prev, pos, full_form, lemma) in ud_tuples if (pos, full_form) not in lexin_full_forms]\n",
    "\n",
    "# Create a new list of tuples consisting of the ones from DSN and the new ones just found in UD.\n",
    "#lexin_ud_no_history = lexin_tuples + list(set(ud_tuples_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set for looking up POS + full form combinations found in DSN.\n",
    "saldom_full_forms = set((pos, full_form) for pos, full_form, _lemma in saldom_tuples)\n",
    "\n",
    "# Create a list of POS + full form + lemma tuples from UD for wich the POS + full form combination\n",
    "# is *not* found in DSN.\n",
    "ud_tuples_unique = [(pos, full_form, lemma) for (_pos_prev, pos, full_form, lemma) in ud_tuples if (pos, full_form) not in saldom_full_forms]\n",
    "\n",
    "# Create a new list of tuples consisting of the ones from DSN and the new ones just found in UD.\n",
    "saldom_ud_no_history = saldom_tuples + list(set(ud_tuples_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', '%', '%'),\n",
       " ('NOUN', '%-', '%'),\n",
       " ('CONJ', '&', '&'),\n",
       " ('ADP', '+', '+'),\n",
       " ('ADP', '-', '-')]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saldom_ud_no_history[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ADJ', 'många', 'många')]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in saldom_ud_no_history if t[0] == 'ADJ' and t[1] == 'många']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Ambiguity\n",
    "We have now removed words with two accepted spellings. Unfortunately, we have at least one more kind of ambiguity left in the data, namely distinct words which share one or more forms. For example, the Danish word \"se\" means *see*. Past tense of \"se\" is \"så\" (somewhat similar to *saw*). But the word \"så\" also has another meaning in Danish, namely *sow*. Consequently, if we are to lemmatize the word \"så\" and do not have any other information, we cannot tell whether the lemma is \"se\" or \"så\". For these situation, it helps if we know the POS tag of the previous word of the sentence. Therefor, we now identify ambiguous words which are still present after the above cleaning of ambiguous words. For these ambiguous words, we then build a list of tuples which include the POS tag of the previous word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_ambiguous_lemmas(forms):\n",
    "    counter = Counter(t[:2] for t in forms)\n",
    "    ambiguous = list(set([key for key in counter if counter[key] > 1]))\n",
    "    return ambiguous\n",
    "\n",
    "#ambiguous = find_ambiguous_lemmas(clean_dsn_ud_no_history)\n",
    "ambiguous = find_ambiguous_lemmas(saldom_ud_no_history)\n",
    "saldom_ud_with_history = [(f'{f[0]}_{f[1]}',) + f[2:] for f in ud_tuples if f[1:3] in ambiguous]\n",
    "len(saldom_ud_with_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Tuples To Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_form(word_class, full_form, lemma):\n",
    "    writer.writerow([word_class, full_form, lemma])\n",
    "\n",
    "with open(PREPARED_FILE, 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile,\n",
    "                        delimiter=\",\",\n",
    "                        quotechar='\"',\n",
    "                        quoting=csv.QUOTE_MINIMAL,\n",
    "                        encoding='utf-8',\n",
    "                        lineterminator='\\n')\n",
    "    \n",
    "    writer.writerow(['word_class', 'full_form', 'lemma'])\n",
    "    \n",
    "    for pos, full_form, lemma in sorted(saldom_ud_no_history, key = lambda x: (x[1:], x[0])):\n",
    "        _write_form(pos, full_form, lemma)\n",
    "    for pos, full_form, lemma in sorted(saldom_ud_with_history, key = lambda x: (x[1:], x[0])):\n",
    "        _write_form(pos, full_form, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
