{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a lemmatizer with Lemmy\n",
    "In this notebook, you will see how to train a lemmatizer using lemmy. It assumes you already have a CSV file of the \n",
    "format *pos*, *full_form*, *lemma*. The previous notebook, *01 prepare*, explains how to create such a file using data from Dansk Sprognævn (DSN) and the Universal Dependency (UD) data.\n",
    "\n",
    "We initially create a train/test split and train on the training data only and then evaluate on the train and test set respectively. We then train again on the entire dataset and save the trained rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from pprint import pformat\n",
    "import pandas as pd\n",
    "from lemmy import Lemmatizer\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s : %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPARED_FILE = \"./data/prepared.csv\"\n",
    "TRAINED_RULES_FILE = \"./data/rules.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(lemmatizer):\n",
    "    examples = [[\"VERB\", \"drak\"], [\"NOUN\", \"kattene\"], [\"NOUN\", \"ukrudtet\"], [\"NOUN\", \"slaraffenlandet\"],\n",
    "                [\"NOUN\", \"alen\"], [\"NOUN\", \"skaber\"], [\"NOUN\", \"venskaber\"], [\"NOUN\", \"tilbageførelser\"],\n",
    "                [\"NOUN\", \"aftenbønnerne\"], [\"NOUN\", \"altankassepassere\"]]\n",
    "    for word_class, full_form in examples:\n",
    "        lemma = lemmatizer.lemmatize(word_class, full_form)\n",
    "        print(\"(%s, %s) -> %s\" % (word_class, full_form, lemma))\n",
    "\n",
    "def calculate_accuracy(lemmatizer, X, y):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    ambiguous = 0\n",
    "\n",
    "    for index in range(len(y)):\n",
    "        word_class, full_form = X[index]\n",
    "        target = y[index]\n",
    "        predicted = lemmatizer.lemmatize(word_class, full_form)\n",
    "        total += 1\n",
    "        if len(predicted) > 1:\n",
    "            ambiguous += 1\n",
    "        elif predicted[0] == target:\n",
    "            correct += 1\n",
    "\n",
    "\n",
    "    print(\"correct:\", correct)\n",
    "    print(\"ambiguous:\", ambiguous)\n",
    "    print(\"total:\", total)\n",
    "    print(\"accuracy:\", correct/total)\n",
    "    print(\"ambiguous%:\", ambiguous/total)\n",
    "    print(\"ambiguous + accuracy:\", (ambiguous+correct)/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename, usecols=[0, 1, 2], keep_default_na=False)\n",
    "    df = df.sample(frac=1, random_state=42) # shuffle rows\n",
    "    X = [(word_class, full_form) for _, (word_class, full_form, _) in df.iterrows()]\n",
    "    y = [lemma for _, (_word_class, _full_form, lemma,) in df.iterrows()]\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(PREPARED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    mask = [False] * len(y)\n",
    "    test_indices = random.sample(range(len(y)), len(y) // 500)\n",
    "    for index in test_indices:\n",
    "        mask[index] = True\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for index, test in enumerate(mask):\n",
    "        if test:\n",
    "            X_test += [X[index]]\n",
    "            y_test += [y[index]]\n",
    "        else:\n",
    "            X_train += [X[index]]\n",
    "            y_train += [y[index]]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "random.seed(42)\n",
    "X_train, y_train, X_test, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete set:    1051863\n",
      "Train set:       1049760\n",
      "Test set:           2103\n"
     ]
    }
   ],
   "source": [
    "print(f\"Complete set: {len(X):10}\")\n",
    "print(f\"Train set:    {len(X_train):10}\")\n",
    "print(f\"Test set:     {len(X_test):10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train temmatizer - training set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : epoch #1: 77567 rules (77567 new) in 4.17s\n",
      "DEBUG : epoch #2: 103334 rules (25767 new) in 4.05s\n",
      "DEBUG : epoch #3: 108183 rules (4849 new) in 4.09s\n",
      "DEBUG : epoch #4: 109247 rules (1064 new) in 4.06s\n",
      "DEBUG : epoch #5: 109566 rules (319 new) in 4.00s\n",
      "DEBUG : epoch #6: 109671 rules (105 new) in 3.93s\n",
      "DEBUG : epoch #7: 109695 rules (24 new) in 3.93s\n",
      "DEBUG : epoch #8: 109703 rules (8 new) in 3.98s\n",
      "DEBUG : epoch #9: 109705 rules (2 new) in 3.96s\n",
      "DEBUG : epoch #10: 109705 rules (0 new) in 4.03s\n",
      "DEBUG : training complete: 109705 rules in 40.36s\n",
      "DEBUG : rules before pruning: 109705\n",
      "DEBUG : used rules: 101430\n",
      "DEBUG : rules after pruning: 101430 (8275 removed)\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 1041250\n",
      "ambiguous: 8510\n",
      "total: 1049760\n",
      "accuracy: 0.991893385154702\n",
      "ambiguous%: 0.008106614845297972\n",
      "ambiguous + accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(lemmatizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 1987\n",
      "ambiguous: 21\n",
      "total: 2103\n",
      "accuracy: 0.9448407037565383\n",
      "ambiguous%: 0.009985734664764621\n",
      "ambiguous + accuracy: 0.9548264384213029\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(lemmatizer, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VERB, drak) -> ['draka']\n",
      "(NOUN, kattene) -> ['kattene']\n",
      "(NOUN, ukrudtet) -> ['ukrudte']\n",
      "(NOUN, slaraffenlandet) -> ['slaraffenland']\n",
      "(NOUN, alen) -> ['al']\n",
      "(NOUN, skaber) -> ['skab']\n",
      "(NOUN, venskaber) -> ['venskab']\n",
      "(NOUN, tilbageførelser) -> ['tilbageførelse']\n",
      "(NOUN, aftenbønnerne) -> ['aftenbønnerne']\n",
      "(NOUN, altankassepassere) -> ['altankassepassera']\n"
     ]
    }
   ],
   "source": [
    "print_examples(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train temmatizer - full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : epoch #1: 77730 rules (77730 new) in 4.39s\n",
      "DEBUG : epoch #2: 103564 rules (25834 new) in 4.59s\n",
      "DEBUG : epoch #3: 108442 rules (4878 new) in 4.39s\n",
      "DEBUG : epoch #4: 109506 rules (1064 new) in 4.40s\n",
      "DEBUG : epoch #5: 109826 rules (320 new) in 4.16s\n",
      "DEBUG : epoch #6: 109931 rules (105 new) in 4.15s\n",
      "DEBUG : epoch #7: 109955 rules (24 new) in 4.09s\n",
      "DEBUG : epoch #8: 109963 rules (8 new) in 4.20s\n",
      "DEBUG : epoch #9: 109965 rules (2 new) in 4.32s\n",
      "DEBUG : epoch #10: 109965 rules (0 new) in 4.15s\n",
      "DEBUG : training complete: 109965 rules in 43.01s\n",
      "DEBUG : rules before pruning: 109965\n",
      "DEBUG : used rules: 101668\n",
      "DEBUG : rules after pruning: 101668 (8297 removed)\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemmatizer.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 1043300\n",
      "ambiguous: 8563\n",
      "total: 1051863\n",
      "accuracy: 0.9918592059992604\n",
      "ambiguous%: 0.00814079400073964\n",
      "ambiguous + accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(lemmatizer, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Learned Rules\n",
    "We now save the learend rules to a Python file which can be copied to the lemmatizer source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_dict(lemmatizer):\n",
    "    \"\"\"Convert the internal defaultdict to a standard dict.\"\"\"\n",
    "    temp = {}\n",
    "    for pos, rules_ in lemmatizer.rules.items():\n",
    "        if pos not in temp:\n",
    "            temp[pos] = {}\n",
    "\n",
    "        for full_form_suffix, lemma_suffixes_ in rules_.items():\n",
    "            temp[pos][full_form_suffix] = lemma_suffixes_\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINED_RULES_FILE, 'w') as file:\n",
    "    file.write(\"# coding: utf-8\\n\")\n",
    "    file.write(\"from __future__ import unicode_literals\\n\")\n",
    "    file.write(\"\\n\\n\")\n",
    "    file.write(\"rules = \" + pformat(_to_dict(lemmatizer), width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
