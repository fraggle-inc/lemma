{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Training\n",
    "In this notebook, we prepare a dataset we can use for training a lemmatizer with lemma.\n",
    "\n",
    "We use two datasets which are both publicly available. The first dataset is the word list from Dansk Sprognævn (DSN). This dataset is freely available but you have to sign a contract with DSN to obtain the file. Please see [www.dsn.dk](https://www.dsn.dk) for more info. The other dataset is the Danish part of the Universal Dependencies (UD). This dataset is open source and available from the [UD repo](https://github.com/UniversalDependencies/UD_Danish) on GitHub.\n",
    "\n",
    "The notebook assumes you have the datasets stored in a subfolder called *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import unicodecsv as csv\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s : %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_TRAIN_FILE = \"./data/UD_Danish/da-ud-train.conllu\"\n",
    "DSN_XML_FILE = \"./data/DSN/RO.iLexdump.m.fuldformer.til.aftagere.xml\"\n",
    "PREPARED_FILE = \"./data/prepared.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse DSN XML data\n",
    "We will now read the DSN data. Because the word classes used in DSN data do not map 1-to-1 to, we need to do some manual mapping. The CLASS_LOOKUP dictionary specifies the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LOOKUP = {\"sb\": [\"NOUN\"],\n",
    "                \"adj\": [\"ADJ\"],\n",
    "                \"adv\": [\"ADV\"],\n",
    "                \"vb\": [\"VERB\"],\n",
    "                \"proprium\": [\"PROPN\"],\n",
    "                \"præp\": [\"ADP\"],\n",
    "                \"udråbsord\": [\"INTJ\"],\n",
    "                \"pron\": [\"PRON\"],\n",
    "                \"talord\": [\"NUM\"],\n",
    "                \"konj\": [\"CONJ\"],\n",
    "                \"romertal\": [\"NUM\"],\n",
    "                \"kolon\": [\"NOUN\"],\n",
    "                \"lydord\": [\"NOUN\"],\n",
    "                \"art\": [\"PRON\"]}\n",
    "\n",
    "def _build_dsn_forms(soup):\n",
    "    unknown_classes = defaultdict(int)\n",
    "    forms = set()\n",
    "    logging.debug(\"finding 'hom' tags\")\n",
    "    homograph_groups = soup.find_all('hom', recursive=True)\n",
    "    for hom_group in tqdm(homograph_groups, leave=False):\n",
    "        for article in hom_group.find_all(recursive=False):\n",
    "            word_class_temp = article.name.split('-')[0]\n",
    "            word_classes = CLASS_LOOKUP.get(word_class_temp, None)\n",
    "\n",
    "            if not word_classes:\n",
    "                unknown_classes[word_class_temp] += 1\n",
    "                continue\n",
    "\n",
    "            head_node = article.find('hoved')\n",
    "            lemma = head_node.find('opslagsord').get_text()\n",
    "            full_forms = article.find('fuldformer')\n",
    "            if full_forms is None:\n",
    "                continue\n",
    "\n",
    "            form_of = head_node.find('form.af')\n",
    "            if form_of:\n",
    "                # The lookup word ('artikel') itself is not the baseform.\n",
    "                 continue\n",
    "\n",
    "\n",
    "            for full_form_tag in full_forms.find_all('ff', recursive=False):\n",
    "                full_form = full_form_tag.get_text()\n",
    "                for word_class in word_classes:\n",
    "                    forms.add((word_class, full_form, lemma))\n",
    "    return sorted(forms, key = lambda x: x[1:]), unknown_classes\n",
    "\n",
    "def _write_form(word_class, full_form, lemma):\n",
    "    writer.writerow([word_class, full_form, lemma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : reading XML...\n"
     ]
    }
   ],
   "source": [
    "logging.debug(\"reading XML...\")\n",
    "soup = BeautifulSoup(open(DSN_XML_FILE), 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : finding 'hom' tags\n",
      "                                                       \r"
     ]
    }
   ],
   "source": [
    "dsn_forms, unknown = _build_dsn_forms(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse UD data\n",
    "We want to learn from both the DSN and UD data. While DSN is the authorative source, UD does contain words not found in DSN. In case of inconsistencies between DSN and UD, we choose DSN over UD.\n",
    "\n",
    "Universal Dependencies (UD) and spaCy use POS-tags such as *DET* and *AUX* which are not used in the DSN word lists. So, we use UD to learn these.\n",
    "\n",
    "For adjectives (*ADJ*), the DSN word lists are incomplete. They do not contain various *degrees* for the adjectives, for example the forms *hurtigere* (faster) and *hurtigst* (fastest).\n",
    "\n",
    "UD contains a large amount of proper nouns (*PROPN*) not found in DSN, specifically personal names. We might as well learn from these as well, so read the entire UD train file.\n",
    "\n",
    "We try to remedy this by learning from UD. Unfortunately, there is some amount inconsistency between DSN and UD. To avoid introducing ambiguity because of this, we will only add data from UD if the full form does not exist in DSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_ud_line(line):\n",
    "    return line.split(\"\\t\")[1:4]\n",
    "\n",
    "def _ud_forms(ud_file, min_freq=1):\n",
    "    counts = {}\n",
    "    pos_prev = \"\"\n",
    "    for line in open(ud_file).readlines():\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.strip() == \"\":\n",
    "            pos_prev = \"\"\n",
    "            continue\n",
    "\n",
    "        orth, lemma, pos = _parse_ud_line(line)\n",
    "        orth = orth.lower()\n",
    "        lemma = lemma.lower()\n",
    "        key = (pos_prev, pos, orth, lemma)\n",
    "        counts[key] = counts.get(key, 0) + 1\n",
    "        pos_prev = pos\n",
    "    \n",
    "    return [key for key in counts if counts[key] >= min_freq]\n",
    "\n",
    "ud_forms = _ud_forms(UD_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'ADP', 'på', 'på'),\n",
       " ('ADP', 'NOUN', 'fredag', 'fredag'),\n",
       " ('NOUN', 'AUX', 'har', 'have'),\n",
       " ('AUX', 'PROPN', 'sid', 'sid'),\n",
       " ('PROPN', 'VERB', 'inviteret', 'invitere')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud_forms[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter UD data\n",
    "We will now filter the word forms read from UD. We do this to avoid introducing ambiguity due to spelling errors and typos in UD.\n",
    "\n",
    "We want to include the following only:\n",
    "1. Any POS + full form combination *not* found in DSN.\n",
    "2. Any POS_PREV + POS + full form combination for which the POS + full form is *ambiguous* in DSN + Step 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn_full_forms = set((pos, full_form) for pos, full_form, _lemma in dsn_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_forms_unique = [(pos, full_form, lemma) for (_pos_prev, pos, full_form, lemma) in ud_forms if (pos, full_form) not in dsn_full_forms]\n",
    "ud_forms_unique = list(set(ud_forms_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsn_ud_no_history = dsn_forms + ud_forms_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3210"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find ambiguous ones\n",
    "counter = Counter(t[:2] for t in dsn_ud_no_history)\n",
    "ambiguous = set([key for key in counter if counter[key] > 1])\n",
    "len(ambiguous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsn_ud_with_history = [(f'{f[0]}_{f[1]}',) + f[2:] for f in ud_forms if f[1:3] in ambiguous]\n",
    "len(dsn_ud_with_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PREPARED_FILE, 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile,\n",
    "                        delimiter=\",\",\n",
    "                        quotechar='\"',\n",
    "                        quoting=csv.QUOTE_MINIMAL,\n",
    "                        encoding='utf-8',\n",
    "                        lineterminator='\\n')\n",
    "    \n",
    "    writer.writerow(['word_class', 'full_form', 'lemma'])\n",
    "    \n",
    "    for pos, full_form, lemma in sorted(dsn_ud_no_history, key = lambda x: x[1:]):\n",
    "        _write_form(pos, full_form, lemma)\n",
    "    for pos, full_form, lemma in sorted(dsn_ud_with_history, key = lambda x: x[1:]):\n",
    "        _write_form(pos, full_form, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknown classes from DSN\n",
    "Finally, we will list word classes found in DSN for which we do not have a mapping to UD POS-tags. Further investigation for these is an area for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fork 361\n",
      "flerord.forb. 196\n",
      "præfiks 58\n",
      "formelt 2\n"
     ]
    }
   ],
   "source": [
    "for key, value in unknown.items():\n",
    "    print(key, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
